# Obsidian Copilot Environment Configuration
# Copy this file to .env and update with your values

# Required: Path to your Obsidian vault (must end with /)
OBSIDIAN_PATH=/path/to/your/obsidian-vault/

# Required: Path to transformer model cache
TRANSFORMER_CACHE=/path/to/.cache/huggingface/hub

# Claude Configuration
# Set to true to use Claude backend instead of OpenAI
USE_CLAUDE_BACKEND=false

# Path to Claude Code CLI (if not in PATH)
# On macOS: usually /usr/local/bin/claude
# On Linux: might be /usr/bin/claude or ~/.local/bin/claude
CLAUDE_CODE_PATH=/usr/local/bin/claude

# Claude model to use (options: claude-3-5-sonnet-20241022, claude-3-opus-20240229, etc.)
CLAUDE_MODEL=claude-3-5-sonnet-20241022

# Maximum context tokens for Claude (Claude supports up to 200K)
MAX_CONTEXT_TOKENS=100000

# OpenAI Configuration (for backward compatibility)
# Your OpenAI API key (if using OpenAI backend)
OPENAI_API_KEY=sk-xxxxxxxxxx

# Docker/Container Configuration
# Choose your container runtime: docker or podman
RUNTIME=docker

# Network configuration
NETWORK=obsidian-copilot
IMAGE_TAG=obsidian-copilot

# Server Configuration
# Backend server URL (for plugin to connect)
BACKEND_URL=http://localhost:8000

# Development Settings
# Enable debug logging
DEBUG=false

# Performance Settings
# Maximum number of chunks to include in context
MAX_CHUNKS=20

# Timeout for Claude generation (in seconds)
GENERATION_TIMEOUT=60